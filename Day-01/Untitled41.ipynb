{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11ee43-53a6-44ce-9372-1dd5045c4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP\n",
    "\n",
    "NLP stands for Natural Language Processing.\n",
    "NLP is a field of artificial intelligence and linguistics that focuses on the interactions between the computer and human languages.\n",
    "It involves developing algorithms and models that enable machines to understand, interpret and generate human language in a way that\n",
    "is both helpful and meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbf7-ad35-4d26-bc70-738482cc7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP Main tasks are\n",
    "\n",
    "1) Text Preprocessing\n",
    "\n",
    "Preprocessing means cleaning the data. In our Text Preprocessing there are three types of preprocessing works.\n",
    "\n",
    "a) Tokenization:\n",
    "\n",
    "Splitting text into words, sentences and other meaningful units.\n",
    "\n",
    "b) Lemmatization/stemming:\n",
    "\n",
    "It Reduces the words to thier base form. for example the word talking is reduced into talk.\n",
    "\n",
    "c) Stopword Removal:\n",
    "\n",
    "It remove the words which doesnt contain the significant meaning example words like the,is etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fed14f-770e-4618-83be-27c1be820ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "2) Text Classification\n",
    "\n",
    "Categorizing text into predefined labels (e.g. sentimental analysis, spam detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b9f9b-7c73-4651-ab72-b1fd24278ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) Named Entity Relationship(NER)\n",
    "\n",
    "Identifying proper names(people,organizations,location) within text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c9948-8afd-401f-97e3-7790df08ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "4) Parts of speech tagging(POS)\n",
    "\n",
    "Identifying the grammatical solution of a sentence(eg. Noun,verb,Adjective etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a49776-3f85-451e-b2e2-bd72c439b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "5) Machine Translation\n",
    "\n",
    "Automatically translate the text from one language to another language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9264147-bfe3-47a0-930b-ad28cb0ca3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "6) Question and Answering (QA):\n",
    "\n",
    "Developing the system that can answer the questions posed in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810857ac-143b-4909-af27-41621134c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "7) Text Generation\n",
    "\n",
    "Creating Coherent text based on the given input (language models like gpt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196596e0-9662-4803-b627-a91e10da28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "8) Sentiment Analysis\n",
    "\n",
    "Determine the Sentiment(Positive, Negative and Neutral) expressed in a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779620f-9739-4ec6-a3f4-7024dcf1cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Techniques and Models:\n",
    "\n",
    "Bag of Words (BoW): Representing text by counting word frequencies, ignoring order.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): Weighs words based on how important they are in a specific document relative to a collection.\n",
    "\n",
    "Word Embeddings: Words represented as vectors in a continuous space (e.g., Word2Vec, GloVe) to capture semantic meaning.\n",
    "\n",
    "Transformers: Advanced deep learning models that capture contextual relationships in text (e.g., BERT, GPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce5ae7-929a-4349-9653-f6ce1188201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Common NLP Applications:\n",
    "Search engines (understanding and ranking results based on queries).\n",
    "Chatbots (providing conversational AI).\n",
    "Voice Assistants (e.g., Siri, Alexa).\n",
    "Text Summarization (extracting key points from long documents).\n",
    "Automatic Translation (e.g., Google Translate).\n",
    "Challenges:\n",
    "Ambiguity: Words having multiple meanings based on context.\n",
    "Sarcasm and Irony: Difficult to detect accurately.\n",
    "Context Understanding: Capturing deeper meaning beyond just words.\n",
    "NLP uses a combination of rule-based methods, statistical models, and machine learning (especially deep learning techniques) to handle these challenges.\n",
    "\n",
    "If you’re looking to learn or implement NLP, tools like Python libraries (NLTK, spaCy, transformers) and pre-trained models are very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f9822-ed91-470f-952f-9b1f82797b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Punkt is a sentence and word tokenizer used in NLTK (Natural Language Toolkit), which is an essential tool for text preprocessing in NLP. \n",
    "                                                                                                          \n",
    "It is a pre-trained tokenizer that is capable of dividing text into sentences and words based on punctuation marks and other syntactical rules.\n",
    "\n",
    "The Punkt tokenizer is trained on a large amount of text data, making it highly accurate at segmenting text into meaningful tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c626216-9be4-4b67-8617-d636f77d83c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PythonTeam\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\PythonTeam\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\PythonTeam\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenizer:\n",
      "['Hello World!', 'My Name is Sai Balaji']\n",
      "/nWord Tokenizer:\n",
      "['Hello', 'World', '!', 'My', 'Name', 'is', 'Sai', 'Balaji']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PythonTeam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nltk.download('punkt')\n",
    "text=\"Hello World! My Name is Sai Balaji\"\n",
    "sentences=sent_tokenize(text)\n",
    "print(\"Sentence Tokenizer:\")\n",
    "print(sentences)\n",
    "words=word_tokenize(text)\n",
    "print(\"/nWord Tokenizer:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f602872-93c2-4a3d-baf7-60021a1bea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Why Use Punkt?\n",
    "\n",
    "Accuracy: Punkt is highly effective because it’s based on machine learning models that understand language-specific punctuation and structure.\n",
    "Ease of Use: It requires no manual configuration or language-specific rules—simply use sent_tokenize() and word_tokenize() \n",
    "for quick and accurate tokenization.\n",
    "If you are working with raw text and need to break it down into smaller parts for analysis, \n",
    "Punkt is one of the best tools available in NLTK for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf49989-1ca8-431c-adae-a5d344b5a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP libraries\n",
    "\n",
    "Library\t        Best For\t                                        Ease of Use\t               Performance\n",
    "NLTK\t        Learning, preprocessing, traditional NLP\t        Beginner\t               Medium\n",
    "spaCy\t        Industrial-strength NLP\t                            Easy\t                   High\n",
    "Transformers\tAdvanced, state-of-the-art tasks\t                Intermediate\t           High\n",
    "TextBlob\t    Quick prototypes, sentiment analysis\t            Beginner\t               Medium\n",
    "Gensim\t        Topic modeling, document similarity\t                Intermediate\t           High\n",
    "Stanza\t        Academic research, multilingual NLP\t                Intermediate\t           High\n",
    "Flair\t        Word embeddings, sentiment analysis\t                Easy\t                   High\n",
    "FastText\t    Fast embeddings, text classification\t            Easy\t                   High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf39101-2aed-4546-a5a6-2bdd5dfbf79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic Text Preprocessing with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88096b-39ed-43d9-879c-911f91326925",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca3eb28-8b76-42f0-a646-e50de47dca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenizer\n",
      "['Hello World!', 'Iam Sai Balaji']\n",
      "\n",
      "Word Tokenizer\n",
      "['Hello', 'World', '!', 'Iam', 'Sai', 'Balaji']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PythonTeam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nltk.download('punkt')\n",
    "text=\"Hello World! Iam Sai Balaji\"\n",
    "sentences=sent_tokenize(text)\n",
    "print(\"Sentence Tokenizer\")\n",
    "print(sentences)\n",
    "words=word_tokenize(text)\n",
    "print(\"\\nWord Tokenizer\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a083c6-ca5e-4894-ac47-7f6fd1c26155",
   "metadata": {},
   "outputs": [],
   "source": [
    "Removing Stopwords\n",
    "Stopwords are common words that don’t add much meaning to the text (e.g., \"the\", \"is\", \"and\"). Removing them helps reduce the noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe61dff-8de7-4d9b-a6b3-a91300a1194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Hello', 'World', '!', 'Iam', 'Sai', 'Balaji']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PythonTeam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_words=[word for word in words if word.lower() not in stop_words]\n",
    "print(f'Filtered Words: {filtered_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9193d-c16f-435d-87a2-b2f1a42a40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemming and Lemmatization\n",
    "Both stemming and lemmatization reduce words to their base form. \n",
    "Stemming simply removes suffixes (e.g., \"running\" -> \"run\"), while lemmatization uses vocabulary and morphological analysis \n",
    "to find the root word (e.g., \"better\" -> \"good\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca366f68-a08e-49cc-8e2c-e355fd3b5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['run', 'talk', 'walk', 'better', 'fli', 'eat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PythonTeam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['running', 'talking', 'walking', 'better', 'fly', 'eating']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "stemmer=PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "words=['running','talking','walking','better','flies','eating']\n",
    "stemmed=[stemmer.stem(word) for word in words]\n",
    "print(f'Stemmed words: {stemmed}')\n",
    "nltk.download('wordnet')\n",
    "lemmatized=[lemmatizer.lemmatize(word) for word in words]\n",
    "print(f'Lemmatized Words: {lemmatized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353e4f5-fc04-4178-81cd-c510e960eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Named Entity Recognition (NER) with spaCy\n",
    "Named Entity Recognition (NER) is used to identify entities like names of people, locations, organizations, and more from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4b4fce-3b89-4de6-8a87-d779cf20118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Balaji, Label: PERSON\n",
      "Entity: Hyderabad, Label: ORG\n",
      "Entity: 2024, Label: DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "text=\"Balaji was born in Hyderabad and his profession is python developer ensured in 2024\"\n",
    "doc=nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text}, Label: {ent.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f326b3d-dee7-4ea6-aeb6-2208acbad9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Part-of-Speech (POS) Tagging with spaCy\n",
    "POS tagging identifies the grammatical role of each word (e.g., noun, verb, adjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d745c861-4dfd-4e2c-b896-f4bd8bb4e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Balaji, Label: PROPN\n",
      "Entity: was, Label: AUX\n",
      "Entity: born, Label: VERB\n",
      "Entity: in, Label: ADP\n",
      "Entity: kolkata, Label: NOUN\n",
      "Entity: and, Label: CCONJ\n",
      "Entity: his, Label: PRON\n",
      "Entity: profession, Label: NOUN\n",
      "Entity: is, Label: AUX\n",
      "Entity: python, Label: ADJ\n",
      "Entity: developer, Label: NOUN\n",
      "Entity: ensured, Label: VERB\n",
      "Entity: in, Label: ADP\n",
      "Entity: 2024, Label: NUM\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "text=\"Balaji was born in kolkata and his profession is python developer ensured in 2024\"\n",
    "doc=nlp(text)\n",
    "for token in doc:\n",
    "    print(f'Entity: {token.text}, Label: {token.pos_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4386a-97fb-4423-b60f-2f58ed5fcc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Text Classification: Sentiment Analysis with Hugging Face Transformers\n",
    "Sentiment analysis classifies the sentiment of a text as positive, negative, or neutral. We'll use the Hugging Face Transformers library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f56350b-c722-441d-8c4e-914cea2db10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PythonTeam\\AppData\\Roaming\\Python\\Python39\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE, Confidence: 0.9997957348823547\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp=pipeline('sentiment-analysis')\n",
    "text='i love samsung'\n",
    "result=nlp(text)\n",
    "print(f'Sentiment: {result[0][\"label\"]}, Confidence: {result[0][\"score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7ade51-c51b-4351-90c6-26f60fea8354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: NEGATIVE, Confidence: 0.9987506866455078\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp=pipeline('sentiment-analysis')\n",
    "text='I Hate Mi'\n",
    "result=nlp(text)\n",
    "print(f'Sentiment: {result[0][\"label\"]}, Confidence: {result[0][\"score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398d0ce-19be-49a8-a34e-cfcb24488614",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Text Generation with GPT (Using Hugging Face)\n",
    "Text generation involves creating new text based on a given prompt. We will use GPT-2 for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf230d21-af71-4879-b026-d9d725d75f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Python is Very Easy Language and it is still in development. It is based on Tidymalloc.\n",
      "\n",
      "Download\n",
      "\n",
      "To read full examples of the language, see this article:\n",
      "\n",
      "You can also download the source code from gulp.com.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator=pipeline('text-generation',model='gpt2')\n",
    "prompt=\"Python is Very Easy Language\"\n",
    "generated_text=generator(prompt,max_length=150,num_return_sequences=1)\n",
    "print(f'Generated Text: {generated_text[0][\"generated_text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61f778d7-e8e5-4c8c-9fcd-4234f5ed6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once Upon a time there was a king Chatrapathi shivaji, who was the son of a nobleman. He was called Chatra, and he was born in the city of Kavar. The king was named Chatri, after the king of the Kavi.\n",
      "\n",
      "The king's name was Chatrath, which means \"the king.\" The name of Chatronath was given to him by the gods. It was said that he had a son named Kri. In the story of his birth, the god Kriti said, \"I will give you a name, a man named Gathath. I will make you king, Gattath.\"\n",
      ". . .\n",
      " (The story is told in a story about the birth of Gati, but it is not mentioned in any of these stories.)\n",
      ", . .\"\n",
      ": The story was told by a young man who had been a student of Surya. Sushil, his father, was an old man, of whom he is the only one who knew the name. When he heard that the word Gatri was being given him, he said to his mother, 'I am Gatsri.'\n",
      "- The tale is said about Gatti, son-in-law of Satyam, king-of-the-Kavars. Satya, in his youth, had married a woman named Sati, daughter of Akshay, her husband. She\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
    "model_name='gpt2'\n",
    "model=GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer=GPT2Tokenizer.from_pretrained(model_name)\n",
    "prompt=\"Once Upon a time there was a king Chatrapathi shivaji\"\n",
    "inputs=tokenizer(prompt,return_tensors='pt')\n",
    "outputs=model.generate(inputs['input_ids'],max_length=300,num_return_sequences=1,no_repeat_ngram_size=2)\n",
    "generated_text=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ccddb6b-8a22-4076-8e0a-7047c856192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian first freedom fighter was uyyalavaada narasimha reddyalavada.\n",
      "\n",
      "The first U.S. fighter to be killed in action was a Uyghur fighter named Umar Farooq. He was killed by a sniper in the middle of a fight with a group of Uygur fighters. The Ugurs were killed when they tried to attack the Ugyur. Ugarit was the first fighter killed during the war. His name was Ughul.\n",
      "\n",
      "\n",
      "Ugaritic language\n",
      ". In the Urdu language, ugar is a word for \"to be\" or \"not to\".\n",
      "\n",
      "\n",
      ". A Ugur is the word used to describe a person who is not a member of the ugur community. It is used in Ur-an-Ugura, Urguru, and Urga-n-Gurur languages.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
    "model_name='gpt2'\n",
    "model=GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer=GPT2Tokenizer.from_pretrained(model_name)\n",
    "prompt=\"Indian first freedom fighter was uyyalavaada narasimha reddy\"\n",
    "inputs=tokenizer(prompt,return_tensors='pt')\n",
    "outputs=model.generate(inputs['input_ids'],max_length=500,num_return_sequences=1,no_repeat_ngram_size=2)\n",
    "generated_text=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0060b94-6a48-413e-b21d-3c6faf48c894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\pythonteam\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\pythonteam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\pythonteam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\pythonteam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\pythonteam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\pythonteam\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96328d60-d7e1-4a44-bc81-3dedb2f3c9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_text: Hello! Thanks for your help\n",
      "Translated_text: హలో!మీ సహాయానికి ధన్యవాదాలు\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='te')\n",
    "print(f'Original_text: {text}')\n",
    "print(f'Translated_text: {translated_text.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73e57762-3151-4960-8801-09832eb3085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_text: Hello! Thanks for your help\n",
      "Translated_text: नमस्ते!आपकी सहायता के लिए धन्यवाद\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='hi')\n",
    "print(f'Original_text: {text}')\n",
    "print(f'Translated_text: {translated_text.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53668c01-3dc4-4e67-a30d-84efc08865c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_text: Hello! Thanks for your help\n",
      "Translated_text: வணக்கம்!உங்கள் உதவிக்கு நன்றி\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='ta')\n",
    "print(f'Original_text: {text}')\n",
    "print(f'Translated_text: {translated_text.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323693b5-9db6-4465-9f9c-02f489bc090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. For Google Translate (googletrans):\n",
    "In the googletrans library, the destination language is provided by the ISO 639-1 language code. Here are some common language codes you might use:\n",
    "\n",
    "Language\tLanguage Code\n",
    "English\t    en\n",
    "Hindi\t    hi\n",
    "Tamil\t    ta\n",
    "Telugu\t    te\n",
    "Kannada\t    kn\n",
    "Malayalam\tml\n",
    "Gujarati\tgu\n",
    "Bengali\t    bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66f8eea6-c288-4c96-a1cc-5035ebaaef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_text: Hello! Thanks for your help\n",
      "Translated_text: வணக்கம்!உங்கள் உதவிக்கு நன்றி\n",
      "Romanized_text: vaṇaghghaṃ!uṅghaḻ udhavighghu naṉṟi\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator = Translator()\n",
    "text = \"Hello! Thanks for your help\"\n",
    "translated_text = translator.translate(text, src='en', dest='Ta')\n",
    "romanized_text = transliterate(translated_text.text, sanscript.TAMIL, sanscript.IAST)\n",
    "print(f'Original_text: {text}')\n",
    "print(f'Translated_text: {translated_text.text}')\n",
    "print(f'Romanized_text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5cbccb20-a30b-4c13-873f-a8cef279cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello! Thanks for your help\n",
      "Translated Text: नमस्ते!आपकी सहायता के लिए धन्यवाद\n",
      "Romanized Text: namaste!āpakī sahāyatā ke lie dhanyavāda\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='hi')\n",
    "romanized_text=transliterate(translated_text.text,sanscript.DEVANAGARI,sanscript.IAST)\n",
    "print(f'Original Text: {text}')\n",
    "print(f'Translated Text: {translated_text.text}')\n",
    "print(f'Romanized Text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f13ca9f3-b7dd-4a25-b821-0af0e04804b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello! Thanks for your help\n",
      "Translated Text: హలో!మీ సహాయానికి ధన్యవాదాలు\n",
      "Romanized Text: halo!mī sahāyāniki dhanyavādālu\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='te')\n",
    "romanized_text=transliterate(translated_text.text,sanscript.TELUGU,sanscript.IAST)\n",
    "print(f'Original Text: {text}')\n",
    "print(f'Translated Text: {translated_text.text}')\n",
    "print(f'Romanized Text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "794dba4e-3679-4535-8e2a-db55e735e295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello! Thanks for your help\n",
      "Translated Text: ಹಲೋ!ನಿಮ್ಮ ಸಹಾಯಕ್ಕಾಗಿ ಧನ್ಯವಾದಗಳು\n",
      "Romanized Text: halo!nimma sahāyakkāgi dhanyavādagaḻu\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='kn')\n",
    "romanized_text=transliterate(translated_text.text,sanscript.KANNADA,sanscript.IAST)\n",
    "print(f'Original Text: {text}')\n",
    "print(f'Translated Text: {translated_text.text}')\n",
    "print(f'Romanized Text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9cfd8440-e48a-47f3-a021-cec18a08d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello! Thanks for your help\n",
      "Translated Text: ഹലോ!നിങ്ങളുടെ സഹായത്തിന് നന്ദി\n",
      "Romanized Text: haleā!niṅṅaḻuṭè sahāyattin nandi\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='ml')\n",
    "romanized_text=transliterate(translated_text.text,sanscript.MALAYALAM,sanscript.IAST)\n",
    "print(f'Original Text: {text}')\n",
    "print(f'Translated Text: {translated_text.text}')\n",
    "print(f'Romanized Text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "969046a5-f591-45aa-90cc-e4147409c768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello! Thanks for your help\n",
      "Translated Text: হ্যালো!আপনার সাহায্যের জন্য ধন্যবাদ\n",
      "Romanized Text: hyālo!āpanāra sāhāyyera janya dhanyavāda\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='bn')\n",
    "romanized_text=transliterate(translated_text.text,sanscript.BENGALI,sanscript.IAST)\n",
    "print(f'Original Text: {text}')\n",
    "print(f'Translated Text: {translated_text.text}')\n",
    "print(f'Romanized Text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0fe1411-1d81-44a2-af1f-1a6bd49abfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello! Thanks for your help\n",
      "Translated Text: હેલો!તમારી સહાય બદલ આભાર\n",
      "Romanized Text: helo!tamārī sahāya badala ābhāra\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='gu')\n",
    "romanized_text=transliterate(translated_text.text,sanscript.GUJARATI,sanscript.IAST)\n",
    "print(f'Original Text: {text}')\n",
    "print(f'Translated Text: {translated_text.text}')\n",
    "print(f'Romanized Text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f56fd26e-31cf-4433-b63c-275b00b5c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello! Thanks for your help\n",
      "Translated Text: ਸਤ ਸ੍ਰੀ ਅਕਾਲ!ਤੁਹਾਡੀ ਮਦਦ ਲਈ ਧੰਨਵਾਦ\n",
      "Romanized Text: sata srī akāla!tuhāḍī madada laī dhaṃnavāda\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "translator=Translator()\n",
    "text=\"Hello! Thanks for your help\"\n",
    "translated_text=translator.translate(text,src='en',dest='pa')  #punjab\n",
    "romanized_text=transliterate(translated_text.text,sanscript.GURMUKHI,sanscript.IAST)\n",
    "print(f'Original Text: {text}')\n",
    "print(f'Translated Text: {translated_text.text}')\n",
    "print(f'Romanized Text: {romanized_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefdad8-13be-415f-ac79-69e6e1e2d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "these are the basic topics of nlp \n",
    "\n",
    "tokenization,stemming,lemmatization,text generation,named entity relationship,text translation, parts of speech, Text Classification and ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
